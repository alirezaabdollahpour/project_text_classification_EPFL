\label{sec:dataset-details}
\subsection{Training files}
\textbf{Small vs.\ full training sets.} The dataset provides two versions of the labeled training data:
\texttt{train\_pos.txt} / \texttt{train\_neg.txt} and
\texttt{train\_pos\_full.txt} / \texttt{train\_neg\_full.txt}.
Each file contains one whitespace-tokenized tweet per line. The \texttt{--use-full} flag selects the full set.

\subsection{Test file format}
\texttt{test\_data.txt} contains one example per line as:
\begin{quote}
\texttt{<Id>,<tweet text>}
\end{quote}
where \texttt{Id} is an integer.

\subsection{Submission format}
All provided training scripts write a submission CSV with header:
\begin{quote}
\texttt{Id,Prediction}
\end{quote}
Predictions are mapped to \(\{-1,+1\}\): negative \(\rightarrow -1\), positive \(\rightarrow +1\).

\subsection{Reproduction commands}
From \texttt{project\_text\_classification\_EPFL/}:
\begin{verbatim}
# (Optional) build vocabulary + co-occurrences for GloVe
./build_vocab.sh
./cut_vocab.sh
python3 pickle_vocab.py
python3 cooc.py
python3 glove_solution.py

# Example transformer training + submission (see run_distilbert.sh)
python3 distilbert_classifier.py --use-full --model-name vinai/bertweet-base \
  --freeze-encoder --val-size 0.05 --epochs 3 \
  --output baseline_submission_distilbert_head.csv
\end{verbatim}
