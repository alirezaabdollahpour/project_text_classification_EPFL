\subsection{Evaluation protocol}
We estimate generalization using a local stratified train/validation split (via \texttt{train\_test\_split}) with validation fraction \texttt{--val-size}. All hyperparameters are selected based on validation performance rather than the online leaderboard.

\subsection{Baselines}
\paragraph{Hashed n-gram classifier.}
We implement a hashing-trick model \citep{weinberger2009featurehashing} where unigrams and optionally bigrams (\texttt{--ngram-max}) are mapped into a fixed feature space (\texttt{--num-features}). A linear classifier (logistic regression via \texttt{EmbeddingBag} accumulation) is trained with mini-batch optimization.

\paragraph{Embedding-bag pooling.}
Using a vocabulary \texttt{vocab.pkl} aligned with an embedding matrix \texttt{embeddings.npy}, we represent each tweet by mean pooling over token embeddings (an \texttt{EmbeddingBag} model). The classifier is either linear or a small MLP (\texttt{--hidden-dim}), with dropout (\texttt{--embedding-dropout}). Embeddings can be frozen or finetuned (\texttt{--embedding-trainable}).

\subsection{GloVe embeddings}
We train in-domain GloVe vectors \citep{pennington2014glove} from tweet co-occurrences. The pipeline builds a vocabulary (frequency cutoff) and a sparse co-occurrence matrix, then optimizes GloVe with SGD updates as implemented in \texttt{glove\_solution.py}. The resulting matrix \texttt{embeddings.npy} is used either for pooled baselines or as auxiliary features in the hybrid transformer model.

\subsection{Transformer fine-tuning and hybrid fusion}
Our main model fine-tunes a tweet-pretrained transformer (BERTweet \citep{nguyen2020bertweet}) for binary classification. The training script supports (i) freezing the encoder and training only the classification head, (ii) optional LoRA adapters \citep{hu2021lora} for parameter-efficient tuning, and (iii) optional SWA \citep{izmailov2018swa}. For hybrid fusion, we compute averaged GloVe embeddings per tweet and concatenate a projected version of this vector with the transformer pooled output before classification; a scale parameter can be estimated automatically from a sample to match feature magnitudes.

\subsection{Implementation and reproducibility}
All training entrypoints write the submission CSV with header \texttt{Id,Prediction}. Example commands (including the hybrid BERTweet + embedding fusion configuration) are provided in \texttt{run\_distilbert.sh}. Appendix~\ref{sec:dataset-details} documents file formats and the submission mapping.
