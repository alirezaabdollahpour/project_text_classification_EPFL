\subsection{Files and formats}
All scripts expect the dataset under \texttt{data/twitter-datasets/} by default (overridable via \texttt{--data-dir}). Training data is provided in two files, one tweet per line:
\texttt{train\_pos.txt} / \texttt{train\_neg.txt} (smaller subset) and
\texttt{train\_pos\_full.txt} / \texttt{train\_neg\_full.txt} (full set enabled via \texttt{--use-full}).
The test set \texttt{test\_data.txt} contains 10{,}000 unlabeled tweets, one per line as \texttt{<Id>,<tweet>}.

\subsection{Tokenization and preprocessing}
Tweets are already whitespace-tokenized in the provided files (tokens separated by single spaces, with emoticons removed). For the transformer pipeline, we optionally apply lightweight normalization before tokenization: HTML unescaping, removal of zero-width characters and leading \texttt{RT}, normalization of mentions to \texttt{@user} and URLs to \texttt{http}, hashtag splitting, and normalization of repeated characters/punctuation; optional emoji demojization and outlier filters are available via flags.

\subsection{Labels and submission mapping}
Internally, our training code uses binary labels \(\{0,1\}\) (negative/positive). Submissions to AIcrowd require \texttt{Id,Prediction} with predictions mapped to \(\{-1,+1\}\), where negative \(\rightarrow -1\) and positive \(\rightarrow +1\).
