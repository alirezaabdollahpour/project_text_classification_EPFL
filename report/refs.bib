@inproceedings{sanh2019distilbert,
  title     = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author    = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle = {NeurIPS Workshop on Energy Efficient Machine Learning and Cognitive Computing},
  year      = {2019}
}

@inproceedings{pennington2014glove,
  title     = {GloVe: Global Vectors for Word Representation},
  author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
  booktitle = {EMNLP},
  year      = {2014}
}

@inproceedings{vaswani2017attention,
  title     = {Attention Is All You Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {NeurIPS},
  year      = {2017}
}

@misc{wolf2020transformers,
  title        = {Transformers: State-of-the-Art Natural Language Processing},
  author       = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Cl{\'e}ment and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Brew, Jamie},
  year         = {2020},
  eprint       = {1910.03771},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL}
}

@misc{hu2021lora,
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  author       = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
  year         = {2021},
  eprint       = {2106.09685},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL}
}

@inproceedings{nguyen2020bertweet,
  title     = {BERTweet: A pre-trained language model for English Tweets},
  author    = {Nguyen, Dat Quoc and Vu, Thanh and Nguyen, Anh Tuan},
  booktitle = {EMNLP (System Demonstrations)},
  year      = {2020}
}


